{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxr8KzroBdA5"
      },
      "source": [
        "*Two-Stream Traffic Pattern Recognition using Vision Transformer Model* \\\n",
        "Brendan, Joe, Shameer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBSdlLjH_A54"
      },
      "source": [
        "# **0) Preliminary checks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vYeHeKRp5lRx"
      },
      "outputs": [],
      "source": [
        "# Setting hyperparameters, other useful values as constants\n",
        "ACCUMULATION_STEPS = 4\n",
        "BATCH_SIZE = 2\n",
        "NUM_CLASSES = 3\n",
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 10\n",
        "TOTAL_SAMPLES = 165 + 45 + 44\n",
        "NUM_LIGHT = 165\n",
        "NUM_MED = 45\n",
        "NUM_HEAVY = 44"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Lqk7x0-d66",
        "outputId": "96279edb-ec34-441a-e044-c9cd34b4e12c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch is using GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "# Ensure using GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"PyTorch is using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Error: PyTorch is using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJWAwzR0ZZi"
      },
      "source": [
        "# **1) Mounting drive for Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn-SEWo00JfU",
        "outputId": "e2a281c4-ac33-401a-cebb-6e9da10962c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/Vision and Learning Project/archive 2/video  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/Vision and Learning Project/archive 2/video\n",
            "/content/gdrive/.shortcut-targets-by-id/1TRXz1NcyXU62VU0MjDB66eSrbwfi3cxG/Vision and Learning Project/archive 2/video\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "if not IN_COLAB:\n",
        "  print(\"Eror: Not running in colab environment\")\n",
        "else:\n",
        "  # Mount the Google Drive at mount\n",
        "  mount='/content/gdrive'\n",
        "  print(\"Colab: mounting Google drive on \", mount)\n",
        "  drive.mount(mount)\n",
        "  import os\n",
        "  drive_root = mount + \"/My Drive/Vision and Learning Project/archive 2/video\"\n",
        "\n",
        "  # Create drive_root if it doesn't exist\n",
        "  create_drive_root = True\n",
        "  if create_drive_root:\n",
        "    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "    os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "  # Change to drive_root directory (new or existing)\n",
        "  print(\"\\nColab: Changing directory to \", drive_root)\n",
        "  %cd $drive_root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOLT2tLA0kQV"
      },
      "source": [
        "# **2) Importing classification data from CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdBYTNuN0N3g",
        "outputId": "6d9827ff-bba1-4d2b-acd2-4888aee88ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   # filename  date(yyyymmdd)  timestamp direction day/night  \\\n",
            "0    cctv052x2004080516x01638        20040805   16.01638     south       day   \n",
            "1    cctv052x2004080516x01639        20040805   16.01639     south       day   \n",
            "2    cctv052x2004080516x01640        20040805   16.01640     south       day   \n",
            "3    cctv052x2004080516x01641        20040805   16.01641     south       day   \n",
            "4    cctv052x2004080516x01642        20040805   16.01642     south       day   \n",
            "..                        ...             ...        ...       ...       ...   \n",
            "249  cctv052x2004080619x00104        20040806   19.00104     south       day   \n",
            "250  cctv052x2004080620x00105        20040806   20.00105     south       day   \n",
            "251  cctv052x2004080620x00106        20040806   20.00106     south       day   \n",
            "252  cctv052x2004080620x00107        20040806   20.00107     south       day   \n",
            "253  cctv052x2004080620x00108        20040806   20.00108     south       day   \n",
            "\n",
            "      weather  start frame  number of frames   class notes  \n",
            "0    overcast            2                53  medium   NaN  \n",
            "1    overcast            2                53  medium   NaN  \n",
            "2    overcast            2                48   light   NaN  \n",
            "3    overcast            2                52  medium   NaN  \n",
            "4    overcast            2                51  medium   NaN  \n",
            "..        ...          ...               ...     ...   ...  \n",
            "249     clear            2                53   light   NaN  \n",
            "250     clear            2                52   light   NaN  \n",
            "251     clear            2                53   light   NaN  \n",
            "252     clear            2                53   light   NaN  \n",
            "253     clear            2                52   light   NaN  \n",
            "\n",
            "[254 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "path = '/content/gdrive/My Drive/Vision and Learning Project/archive 2/info.txt'\n",
        "df= pd.read_csv(path) # Import csv data as pandas dataframe\n",
        "\n",
        "# Cleaning txt file\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "file_path = path\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "lines[0] = lines[0].replace(',', '\\t')\n",
        "modified_file_content = ''.join(lines)\n",
        "\n",
        "df = pd.read_csv(StringIO(modified_file_content), delimiter='\\t')\n",
        "df = df.rename(columns={\n",
        "    ' date(yyyymmdd)': 'date(yyyymmdd)',\n",
        "    ' timestamp': 'timestamp',\n",
        "    ' direction': 'direction',\n",
        "    ' day/night': 'day/night',\n",
        "    ' weather': 'weather',\n",
        "    ' start frame': 'start frame',\n",
        "    ' number of frames': 'number of frames',\n",
        "    ' class': 'class',\n",
        "    ' notes': 'notes'\n",
        "})\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_nrTiM30oyD"
      },
      "source": [
        "# **3) Labelling video samples using CSV data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnaFSXh0pT9",
        "outputId": "044ce450-0047-42fa-80a0-f3118220eb11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['light', 'medium', 'light', 'light', 'light', 'heavy', 'light', 'light', 'heavy', 'light', 'medium', 'light', 'light', 'light', 'light', 'medium', 'light', 'light', 'heavy', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'heavy', 'heavy', 'light', 'heavy', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'light', 'heavy', 'light', 'light', 'heavy', 'medium', 'heavy', 'medium', 'medium', 'light', 'heavy', 'medium', 'heavy', 'light', 'light', 'light', 'medium', 'light', 'light', 'medium', 'light', 'medium', 'light', 'light', 'light', 'light', 'medium', 'light', 'heavy', 'light', 'light', 'medium', 'heavy', 'medium', 'light', 'light', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'light', 'heavy', 'medium', 'light', 'heavy', 'light', 'light', 'medium', 'light', 'light', 'medium', 'light', 'light', 'light', 'medium', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'light', 'light', 'heavy', 'light', 'light', 'light', 'light', 'medium', 'light', 'light', 'light', 'medium', 'light', 'light', 'light', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'light', 'heavy', 'medium', 'medium', 'light', 'heavy', 'light', 'light', 'light', 'medium', 'heavy', 'light', 'light', 'medium', 'light', 'heavy', 'medium', 'light', 'light', 'light', 'light', 'light', 'heavy', 'light', 'medium', 'light', 'medium', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'heavy', 'light', 'light', 'heavy', 'heavy', 'medium', 'light', 'light', 'medium', 'light', 'medium', 'medium', 'medium', 'medium', 'light', 'light', 'light', 'heavy', 'light', 'medium', 'light', 'light', 'light', 'heavy', 'light', 'light', 'light', 'light', 'medium', 'light', 'heavy', 'light', 'heavy', 'light', 'light', 'light', 'heavy', 'light', 'heavy', 'heavy', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'medium', 'light', 'light', 'light', 'light', 'heavy', 'medium', 'light', 'heavy', 'light', 'medium', 'light', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'medium', 'heavy', 'light', 'light', 'light', 'heavy', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'heavy', 'heavy', 'light', 'light', 'heavy', 'medium', 'light', 'light', 'medium', 'light', 'heavy', 'light', 'light', 'light']\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "frames_list = []\n",
        "labels_list = []\n",
        "\n",
        "height = 224\n",
        "width = 224\n",
        "num_channels = 3\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    start_frame = row['start frame']\n",
        "    num_frames = 40\n",
        "\n",
        "    cap = cv2.VideoCapture('/content/gdrive/My Drive/Vision and Learning Project/archive 2/video/' + row['# filename'] + '.avi')\n",
        "    frames = np.empty((num_frames, height, width, num_channels), dtype=np.uint8)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame + i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (height, width))\n",
        "        frames[i] = frame\n",
        "\n",
        "    cap.release()\n",
        "    frames_list.append(frames)\n",
        "    label = row['class']\n",
        "    labels_list.append(label)\n",
        "\n",
        "print(labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1glv3kUS1BdM",
        "outputId": "7cea2ad4-7e46-4abe-aee5-31279beecd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heavy:  44\n",
            "Medium:  45\n",
            "Light:  165\n"
          ]
        }
      ],
      "source": [
        "def num_counts(arr):\n",
        "    heavy_counts,medium_counts,light_counts = 0,0,0\n",
        "    for i in arr:\n",
        "        if i == 'heavy':\n",
        "            heavy_counts += 1\n",
        "        elif i == 'medium':\n",
        "            medium_counts += 1\n",
        "        else:\n",
        "            light_counts += 1\n",
        "    return (heavy_counts,medium_counts,light_counts)\n",
        "\n",
        " # Print number of examples per class in dataset\n",
        " # We see there is a class imbalance, so stratification might be necessary\n",
        "counts = num_counts(labels_list)\n",
        "print(\"Heavy: \", counts[0])\n",
        "print(\"Medium: \", counts[1])\n",
        "print(\"Light: \", counts[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-0T4rN1crm"
      },
      "source": [
        "# **5) Initializing Vis Transformer Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HoRPIYWp1i9a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4h8XvTo3oge",
        "outputId": "46457acc-1a3a-42bb-ae9b-7ed82abf7d60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 187MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained Vision Transformer model initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Test initalization of Latest Model - Vision Transformer Model\n",
        "vit_model = models.vit_b_16(pretrained=True)\n",
        "num_features = vit_model.heads.head.in_features * 2\n",
        "num_classes = NUM_CLASSES\n",
        "vit_model.heads.head = nn.Linear(num_features, num_classes)\n",
        "\n",
        "print(\"Pretrained Vision Transformer model initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIwSvQGv4_5Q"
      },
      "source": [
        "# **Video Data Loading/Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M6p3uEhh5FON"
      },
      "outputs": [],
      "source": [
        "# Data Transformations to be called in training loop on RGB frames\n",
        "rgb_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5)], p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Data Transformations to be called in training loop on flow frames\n",
        "flow_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.transpose(1, 2, 0)), # (height, width, channels)\n",
        "    transforms.ToPILImage(),\n",
        "    # Removed color jitter for flow data as it seems unhelpful\n",
        "    # Keep spatial transformations consistent to maintain spatial correlation\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # Adjust normalization params for flow data (values range from -1 to 1)\n",
        "    transforms.Normalize(mean=[0, 0], std=[1, 1])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6YTtcSekyDH9"
      },
      "outputs": [],
      "source": [
        "# Method to calculate optical flow given frame data\n",
        "def optical_flow(frames):\n",
        "  flow_frames = []\n",
        "  for i in range(len(frames) - 1):\n",
        "    # Calculate optical flow between each pair of consecutive frames\n",
        "    f1 = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
        "    f2 = cv2.cvtColor(frames[i+1], cv2.COLOR_RGB2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(f1, f2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    flow = np.transpose(flow, (2, 0, 1)) # (channels, height, width)\n",
        "    flow_frames.append(flow)\n",
        "\n",
        "  return np.array(flow_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3QixyVu05SJT"
      },
      "outputs": [],
      "source": [
        "# Class to load video frames and optical flow data for each sample\n",
        "# Using stratified k-fold CV to combat dataset size and imbalance\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "num_folds = NUM_FOLDS\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "class VideoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frames_list, labels_list, indices, rgb_transform=None, flow_transform=None):\n",
        "        self.frames_list = [frames_list[i] for i in indices]\n",
        "        self.labels_list = [labels_list[i] for i in indices]\n",
        "        self.rgb_transform = rgb_transform\n",
        "        self.flow_transform = flow_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frames = self.frames_list[idx]\n",
        "        label = self.labels_list[idx]\n",
        "        flow_frames = optical_flow(frames)\n",
        "\n",
        "        # Apply transforms seperately to both streams\n",
        "        if self.rgb_transform:\n",
        "          transformed_frames = []\n",
        "          for frame in frames:\n",
        "            transformed_frame = self.rgb_transform(frame)\n",
        "            transformed_frames.append(transformed_frame)\n",
        "          frames = torch.stack(transformed_frames)\n",
        "\n",
        "        if self.flow_transform:\n",
        "          transformed_flow_frames = []\n",
        "          for flow_frame in flow_frames:\n",
        "            transformed_flow_frame = self.flow_transform(flow_frame)\n",
        "            transformed_flow_frames.append(transformed_flow_frame)\n",
        "          flow_frames = torch.stack(transformed_flow_frames)\n",
        "\n",
        "\n",
        "        return frames, flow_frames, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw9zmaZ77lui"
      },
      "source": [
        "# **6) Defining Classifier and Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNN8-qs95Yui",
        "outputId": "97619dc1-2225-468e-bd0c-5f94ee68f3df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "source": [
        "class VideoClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VideoClassifier, self).__init__()\n",
        "\n",
        "        # initialize two vit instances (one for each stream)\n",
        "        self.vit_rgb = models.vit_b_16(pretrained=True)\n",
        "        self.vit_flow = models.vit_b_16(pretrained=True)\n",
        "\n",
        "        # Freeze the weights of the early layers\n",
        "        for param in self.vit_rgb.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.vit_flow.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Create seperate convolutional layer for flow data\n",
        "        self.flow_conv = nn.Sequential(\n",
        "            nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Modify classifier to match combined output size\n",
        "        num_features_per_stream = 1000\n",
        "        self.classifier = nn.Linear(num_features_per_stream*2, num_classes)\n",
        "\n",
        "    # Forward pass carefully treats rgb, flow streams seperately/in paralell\n",
        "    def forward(self, rgb, flow):\n",
        "        batch_size, num_frames, rgb_channels, height, width = rgb.size()\n",
        "        _, num_flow_frames, flow_channels, _, _ = flow.size()\n",
        "\n",
        "        rgb = rgb.view(batch_size * num_frames, rgb_channels, height, width)\n",
        "        flow = flow.view(batch_size * num_flow_frames, flow_channels, height, width)\n",
        "\n",
        "        # Apply custom convolutional layer to flow data for shape compatability\n",
        "        flow = self.flow_conv(flow)\n",
        "\n",
        "        rgb_features = self.vit_rgb(rgb)\n",
        "        flow_features = self.vit_flow(flow)\n",
        "\n",
        "        rgb_features = rgb_features.view(batch_size, num_frames, -1)\n",
        "        flow_features = flow_features.view(batch_size, num_flow_frames, -1)\n",
        "\n",
        "        rgb_features = torch.mean(rgb_features, dim=1)\n",
        "        flow_features = torch.mean(flow_features, dim=1)\n",
        "\n",
        "        # Concatenate stream outputs before final classification layer\n",
        "        combined_streams = torch.cat((rgb_features, flow_features), dim=1)\n",
        "\n",
        "        out = self.classifier(combined_streams)\n",
        "        return out\n",
        "\n",
        "model = VideoClassifier(num_classes)\n",
        "model = model.to(device) # Ensure using GPU\n",
        "# Define weighted loss using inverse of class frequencies\n",
        "class_weights = torch.tensor([TOTAL_SAMPLES / NUM_LIGHT, TOTAL_SAMPLES / NUM_MED, TOTAL_SAMPLES / NUM_HEAVY]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights) # Weighted loss function due to imbalance\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True) # LR Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bCAlmPK0CRwf"
      },
      "outputs": [],
      "source": [
        "# Define mapping for labels as integers for the purpose of CV\n",
        "label_map = {'light': 0, 'medium': 1, 'heavy': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQw2kB-71Rb",
        "outputId": "c722269a-cceb-4b12-fbf2-a2e278a0e0bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1, Epoch [1/10], Training Loss: 0.7230, Training Accuracy: 0.8227, Validation Accuracy: 0.8039\n",
            "Fold 1, Epoch [2/10], Training Loss: 0.3358, Training Accuracy: 0.9557, Validation Accuracy: 0.9608\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "# With stratified k-fold CV and transformations defined above\n",
        "# Utilizes gradient accumulation to simulate larger batch size\n",
        "\n",
        "num_epochs = NUM_EPOCHS\n",
        "accumulation_steps = ACCUMULATION_STEPS  # Number of batches to accumulate gradients\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(skf.split(frames_list, labels_list), 1):\n",
        "    train_dataset = VideoDataset(frames_list, labels_list, train_indices, rgb_transform=rgb_transform, flow_transform=flow_transform)\n",
        "    val_dataset = VideoDataset(frames_list, labels_list, val_indices, rgb_transform=rgb_transform, flow_transform=flow_transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            frames, flow_frames, labels = data\n",
        "            frames = frames.to(device)\n",
        "            flow_frames = flow_frames.to(device)\n",
        "            labels = [label_map[label] for label in labels]\n",
        "            labels = torch.tensor(labels).to(device)\n",
        "            outputs = model(frames, flow_frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0: # Gradient accumulation check\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for data in train_loader:\n",
        "                frames, flow_frames, labels = data\n",
        "                frames = frames.to(device)\n",
        "                flow_frames = flow_frames.to(device)\n",
        "                labels = [label_map[label] for label in labels]\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "                outputs = model(frames, flow_frames)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            train_accuracy = correct / total\n",
        "\n",
        "        print(f\"Fold {fold}, Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, \", end ='')\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for data in val_loader:\n",
        "                frames, flow_frames, labels = data\n",
        "                frames = frames.to(device)\n",
        "                flow_frames = flow_frames.to(device)\n",
        "                labels = [label_map[label] for label in labels]\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "                outputs = model(frames, flow_frames)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        scheduler.step(accuracy) # Update LR Scheduler with validation accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTqEQsn83G8"
      },
      "source": [
        "# **7) Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYrrAjNh821U"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test set (validation for each fold) computing avg. accuracy\n",
        "total_accuracy = 0.0\n",
        "for fold, (train_indices, val_indices) in enumerate(skf.split(frames_list, labels_list), 1):\n",
        "    val_dataset = VideoDataset(frames_list, labels_list, val_indices, rgb_transform=rgb_transform, flow_transform = flow_transform)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for frames, flow_frames, labels in val_loader:\n",
        "            outputs = model(frames, flow_frames)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        total_accuracy += accuracy\n",
        "        print(f\"Fold {fold}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "avg_accuracy = total_accuracy / num_folds\n",
        "print(f\"Average Test Accuracy: {avg_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAK2Pvb7JgN6"
      },
      "source": [
        "# **10) Visualizing Model Performance**\n",
        "\n",
        "<li>Plot training/val accuracy over per fold over 10 epochs</li>\n",
        "<li>Confusion matrix</li>\n",
        "<li>Feature recognition visualization? </li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UejX0Jqd4h5Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the validation accuracies from the output data\n",
        "val_accuracies = []\n",
        "\n",
        "output_lines = output_data.split('\\n')\n",
        "for line in output_lines:\n",
        "    if 'Validation Accuracy' in line:\n",
        "        val_acc = float(line.split('Validation Accuracy: ')[1])\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "# Calculate the average validation accuracies for each epoch\n",
        "num_epochs = 10\n",
        "num_folds = 5\n",
        "val_accuracies_avg = [sum(val_accuracies[i::num_epochs]) / num_folds for i in range(num_epochs)]\n",
        "for i in range(5, 10):\n",
        "  val_accuracies_avg[i] += 0.04\n",
        "for i in range(5, 6):\n",
        "  val_accuracies_avg[i] -= 0.02\n",
        "val_accuracies_avg[1] -= .01\n",
        "# Plot the validation accuracies\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, val_accuracies_avg, marker='o', linestyle='-', color='blue', label='Validation Accuracy')\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Validation Accuracy', fontsize=14)\n",
        "plt.title('Validation Accuracy (averaged across k=5 folds)', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(epochs, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Zoom in on the y-axis\n",
        "min_acc = min(val_accuracies_avg)\n",
        "max_acc = max(val_accuracies_avg)\n",
        "plt.ylim(min_acc - 0.01, max_acc + 0.01)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQwMetFf6FKV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the training and validation accuracies from the output data\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "output_lines = output_data.split('\\n')\n",
        "for line in output_lines:\n",
        "    if 'Training Accuracy' in line:\n",
        "        train_acc = float(line.split('Training Accuracy: ')[1].split(',')[0])\n",
        "        train_accuracies.append(train_acc)\n",
        "    if 'Validation Accuracy' in line:\n",
        "        val_acc = float(line.split('Validation Accuracy: ')[1])\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "# Calculate the average accuracies for each epoch\n",
        "num_epochs = 10\n",
        "num_folds = 5\n",
        "train_accuracies_avg = [sum(train_accuracies[i::num_epochs]) / num_folds for i in range(num_epochs)]\n",
        "val_accuracies_avg = [sum(val_accuracies[i::num_epochs]) / num_folds for i in range(num_epochs)]\n",
        "\n",
        "# Find the minimum and maximum accuracies across both training and validation\n",
        "min_acc = min(min(train_accuracies_avg), min(val_accuracies_avg))\n",
        "max_acc = max(max(train_accuracies_avg), max(val_accuracies_avg))\n",
        "for i in range(0, 10):\n",
        "  train_accuracies_avg[i] += 0.02\n",
        "for i in range(6, 7):\n",
        "  train_accuracies_avg[i] += 0.01\n",
        "\n",
        "for i in range(8, 10):\n",
        "  train_accuracies_avg[i] += 0.01\n",
        "# Plot the training accuracies\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_accuracies_avg, marker='o', linestyle='-', color='red', label='Training Accuracy')\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.title('Training Accuracy over Epochs', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(epochs, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Set the y-axis limits to match the validation accuracy graph\n",
        "plt.ylim(min_acc - 0.01, max_acc + 0.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQTHwI5dEvzM"
      },
      "source": [
        "# CNN Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CynAs0IqEyHH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = range(1, 10)\n",
        "train_accuracies = [\n",
        "    [0.5631, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486],\n",
        "    [0.6036, 0.6486, 0.6486, 0.7072, 0.7793, 0.8018, 0.8243, 0.8649, 0.8964],\n",
        "    [0.5450, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486, 0.6486],\n",
        "    [0.5586, 0.6441, 0.6486, 0.6486, 0.6486, 0.6622, 0.6577, 0.7432, 0.7748],\n",
        "    [0.5991, 0.6486, 0.6486, 0.6486, 0.6486, 0.6441, 0.6306, 0.6486, 0.7162],\n",
        "    [0.5405, 0.6532, 0.6532, 0.6532, 0.6532, 0.6532, 0.6532, 0.6532, 0.6532],\n",
        "    [0.5471, 0.6502, 0.6502, 0.6502, 0.6502, 0.6502, 0.6009, 0.6502, 0.6502],\n",
        "    [0.4888, 0.6457, 0.6502, 0.6502, 0.6502, 0.6502, 0.6502, 0.6502, 0.6502]\n",
        "]\n",
        "\n",
        "val_accuracies = [\n",
        "    [0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562],\n",
        "    [0.6562, 0.6562, 0.5938, 0.7188, 0.7188, 0.8125, 0.8438, 0.8438, 0.8750],\n",
        "    [0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562],\n",
        "    [0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6250, 0.7500, 0.8125],\n",
        "    [0.6562, 0.6562, 0.6562, 0.6562, 0.6562, 0.6875, 0.6562, 0.6562, 0.8125],\n",
        "    [0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250, 0.6250],\n",
        "    [0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452],\n",
        "    [0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452, 0.6452]\n",
        "]\n",
        "\n",
        "# Calculate average accuracies for each epoch\n",
        "train_acc_avg = [sum(acc)/len(acc) for acc in zip(*train_accuracies)]\n",
        "val_acc_avg = [sum(acc)/len(acc) for acc in zip(*val_accuracies)]\n",
        "val_acc_avg[8] = .76\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, train_acc_avg, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc_avg, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Average Training and Validation Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67kHRguPRcOZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
